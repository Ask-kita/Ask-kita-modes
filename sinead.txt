University College London Motion Input 3 is the next edition of our touch less computing work at University College London in collaboration with Microsoft and Intel. There are 54 students and 5 academics who have worked on the project this academic year.







This project is a webcam based software technology, building upon several open-source machine learning solutions and computer vision techniques. It gives estimates on human movements and articulation as inputs to a computer.







It includes recognition of hands and wrist landmark points, skeletal body joints, head, eyes and facial movements, in conjunction with federated speech.







These inputs are relayed into existing operating systems as near real-time user interactions through gesture mapping to keyboard, mouse and joypad commands. The software has immediate uses in general creativity, fitness and enjoyment of computing, with current day computers and existing software.







It has many potential applications for accessibility, population health, clinical environments and physical therapy. There are also several new inventions as new interaction metaphors in using computers.



The software works with Windows 10 and 11 and there are versions being developed for Mac, Raspberry Pi, Linux and Android.







We will now show several user interface gestures.